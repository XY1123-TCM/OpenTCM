import os
import pandas as pd
import numpy as np
import networkx as nx
from typing import List, Dict, Any, Tuple, Optional
# from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
from tqdm import tqdm
import requests
import json
import re
from dotenv import load_dotenv
import time

load_dotenv()

# API config, take kimi for example
MOONSHOT_API_KEY = os.getenv("MOONSHOT_API_KEY")
MOONSHOT_API_URL = "https://api.moonshot.cn/v1/chat/completions"

class TCMKnowledgeGraph:
    def __init__(self, csv_path: str = None):
        self.graph = nx.MultiDiGraph()
        self.triples_with_source = [] 
        self.relation_types = set()
        self.entity_types = {}
        self.treatment_plans_details = {}

        if csv_path:
            self.load_from_csv(csv_path)

    def load_from_csv(self, csv_path: str):
        try:
            df = pd.read_csv(csv_path)
            print(f"æˆåŠŸåŠ è½½CSVæ–‡ä»¶ï¼Œå…±æœ‰{len(df)}æ¡ä¸‰å…ƒç»„æ•°æ®")

            required_columns = ['Subject', 'Predicate', 'Object']
            for col in required_columns:
                if col not in df.columns:
                    raise ValueError(f"CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦çš„åˆ—ï¼š{col}")

            for _, row in tqdm(df.iterrows(), total=len(df), desc="æ„å»ºçŸ¥è¯†å›¾è°±"):
                subject = str(row['Subject']).strip()
                predicate = str(row['Predicate']).strip()
                obj = str(row['Object']).strip()

                source_book = str(row.get('SourceBookName', '')).strip() 
                source_chapter = str(row.get('SourceChapterName', '')).strip()

                if source_book.lower() == 'nan': source_book = ''
                if source_chapter.lower() == 'nan': source_chapter = ''

                source_info_str = "æœªçŸ¥æ¥æº"
                book_display = ""
                chap_display = ""

                if source_book:
                    book_display = f"ã€Š{source_book}ã€‹"
                if source_chapter:
                    chap_display = source_chapter

                if book_display and chap_display:
                    source_info_str = f"{book_display} - {chap_display}"
                elif book_display:
                    source_info_str = book_display
                elif chap_display: 
                    source_info_str = chap_display
                
                self.add_triple(subject, predicate, obj, source_book, source_chapter, source_info_str)

            print(f"æˆåŠŸæ„å»ºçŸ¥è¯†å›¾è°±ï¼Œå…±æœ‰{len(self.graph.nodes)}ä¸ªèŠ‚ç‚¹ï¼Œ{len(self.graph.edges)}æ¡è¾¹")
            print(f"å…³ç³»ç±»å‹ï¼š{self.relation_types}")

        except Exception as e:
            print(f"åŠ è½½CSVæ–‡ä»¶å¤±è´¥ï¼š{e}")
            raise 

    
    def add_triple(self, subject: str, predicate: str, obj: str, 
                   source_book: str = "æœªçŸ¥æ¥æº", source_chapter: str = "æœªçŸ¥æ¥æº",
                   source_info_str: str = "æœªçŸ¥æ¥æº"):
        self.graph.add_node(subject)
        self.graph.add_node(obj)
        
        self.graph.add_edge(subject, obj, relation=predicate, source=source_info_str)
        self.relation_types.add(predicate)
        self.triples_with_source.append((subject, predicate, obj, source_book, source_chapter, source_info_str))

        
        treatment_related_predicates = ["ä½¿ç”¨è‰è¯", "å‰‚é‡", "åˆ¶å¤‡æ–¹æ³•", "å¤‡æ³¨", "å…·æœ‰ç—‡çŠ¶", "æè¿°æ²»ç–—æ–¹æ¡ˆ", "æ²»ç–—ç–¾ç—…", "æ²»ç–—ç—‡çŠ¶"]
        if predicate in treatment_related_predicates and "æ²»ç–—æ–¹æ¡ˆ" in subject:
            if subject not in self.treatment_plans_details:
                self.treatment_plans_details[subject] = {}
            if predicate not in self.treatment_plans_details[subject]:
                self.treatment_plans_details[subject][predicate] = []
           
            self.treatment_plans_details[subject][predicate].append({"value": obj, "source": source_info_str})

   
    def get_entity_relations(self, entity: str, specific_relations: Optional[List[str]] = None) -> List[Tuple[str, str, str, str]]:
        relations = []
        
        for s, p, o, book, chap, source_str in self.triples_with_source:
            if s == entity or o == entity:
                if specific_relations is None or p in specific_relations:
                    relations.append((s, p, o, source_str))
        return relations

   
    def get_treatment_plan_full_details(self, plan_name: str) -> Dict[str, Any]:
        details = {
            "åç§°": plan_name, "ç»„æˆ": [], "åˆ¶å¤‡æ–¹æ³•": [], "åŠŸèƒ½ä¸»æ²»": [],
            "å¤‡æ³¨": [], "ç›¸å…³ç—‡çŠ¶": [], "æ²»ç–—ç–¾ç—…": [], "æ¥æºä¿¡æ¯": set() 
        }

        # 1. Retrieve from preprocessed dictionary and collect sources
        if plan_name in self.treatment_plans_details:
            for pred, obj_source_list in self.treatment_plans_details[plan_name].items():
                for item in obj_source_list:
                    obj_val = item["value"]
                    source_str = item["source"]
                    if source_str != "æœªçŸ¥æ¥æº": details["æ¥æºä¿¡æ¯"].add(source_str)

                    if pred == "ä½¿ç”¨è‰è¯":
                        details["ç»„æˆ"].append({"è¯æ": obj_val, "å‰‚é‡": "æœªçŸ¥", "æ¥æº": source_str})
                    elif pred == "åˆ¶å¤‡æ–¹æ³•": details["åˆ¶å¤‡æ–¹æ³•"].append({"value": obj_val, "source": source_str})
                    elif pred == "å¤‡æ³¨": details["å¤‡æ³¨"].append({"value": obj_val, "source": source_str})
                    elif pred == "å…·æœ‰ç—‡çŠ¶": details["ç›¸å…³ç—‡çŠ¶"].append({"value": obj_val, "source": source_str})
                    elif pred == "æ²»ç–—ç–¾ç—…": details["åŠŸèƒ½ä¸»æ²»"].append({"value": obj_val, "source": source_str})


        # 2. Obtain more comprehensive information through graph traversal and extract sources from edge attributes
        for u, target, edge_data in self.graph.out_edges(plan_name, data=True):
            if u != plan_name: continue 
            
            relation = edge_data['relation']
            source_str = edge_data.get('source', "æœªçŸ¥æ¥æº")
            if source_str != "æœªçŸ¥æ¥æº": details["æ¥æºä¿¡æ¯"].add(source_str)

            if relation == "ä½¿ç”¨è‰è¯":
                if not any(d["è¯æ"] == target for d in details["ç»„æˆ"]):
                    details["ç»„æˆ"].append({"è¯æ": target, "å‰‚é‡": "æœªçŸ¥", "æ¥æº": source_str})
            elif relation == "åˆ¶å¤‡æ–¹æ³•" and not any(d["value"] == target for d in details["åˆ¶å¤‡æ–¹æ³•"]):
                details["åˆ¶å¤‡æ–¹æ³•"].append({"value": target, "source": source_str})
            elif relation == "å¤‡æ³¨" and not any(d["value"] == target for d in details["å¤‡æ³¨"]):
                details["å¤‡æ³¨"].append({"value": target, "source": source_str})
            elif relation == "æ²»ç–—ç–¾ç—…" and not any(d["value"] == target for d in details["åŠŸèƒ½ä¸»æ²»"]):
                details["åŠŸèƒ½ä¸»æ²»"].append({"value": target, "source": source_str})
            elif relation == "æ²»ç–—ç—‡çŠ¶" and not any(d["value"] == target for d in details["ç›¸å…³ç—‡çŠ¶"]):
                details["ç›¸å…³ç—‡çŠ¶"].append({"value": target, "source": source_str})

        # For each medicinal herb, try to search for its "dosage" information and its source
        updated_composition = []
        for herb_item in details["ç»„æˆ"]:
            herb_name = herb_item["è¯æ"]
            herb_dosages_with_source = []
            # Find the source of the triplet (herb name, dosage, dosage-value)
            for s_h, p_h, o_h, book_h, chap_h, source_str_h in self.triples_with_source:
                if s_h == herb_name and p_h == "å‰‚é‡":
                    herb_dosages_with_source.append({"value": o_h, "source": source_str_h})
                    if source_str_h != "æœªçŸ¥æ¥æº": details["æ¥æºä¿¡æ¯"].add(source_str_h)
            
            if herb_dosages_with_source:
                # Simplification: If a medicinal herb has multiple dose records, merge them, and also merge the sources or take the most common ones
                herb_item["å‰‚é‡"] = "; ".join([ds["value"] for ds in herb_dosages_with_source])
                # Simply add these dosage sources to the sources of medicinal herbs, or use a unified list
                herb_item["å‰‚é‡æ¥æº"] = list(set(ds["source"] for ds in herb_dosages_with_source if ds["source"] != "æœªçŸ¥æ¥æº"))

            updated_composition.append(herb_item)
        details["ç»„æˆ"] = updated_composition
        
        details["æ¥æºä¿¡æ¯"] = sorted(list(details["æ¥æºä¿¡æ¯"])) 
        details = {k: v for k, v in details.items() if v} 
        return details

    def search_by_keyword(self, keyword: str) -> List[str]:
        matched_entities = []
        for entity in self.graph.nodes():
            if keyword.lower() == str(entity).lower() or keyword.lower() in str(entity).lower():
                matched_entities.append(str(entity))
        return list(set(matched_entities))


    def get_related_entities(self, entity: str, relation_type: Optional[str] = None, max_depth: int = 1) -> List[str]:
        if entity not in self.graph.nodes():
            return []
        visited = set([entity])
        queue = [(entity, 0)]
        related_entities = []
        while queue:
            current_entity, depth = queue.pop(0)
            if depth >= max_depth:
                continue
            # For outgoing edges
            for _, neighbor, edge_data in self.graph.out_edges(current_entity, data=True):
                if relation_type and edge_data.get('relation') != relation_type:
                    continue
                if neighbor not in visited:
                    visited.add(neighbor)
                    queue.append((neighbor, depth + 1))
                    related_entities.append(neighbor)
            # For incoming edges (optional, depending on definition of "related")
            for source, _, edge_data in self.graph.in_edges(current_entity, data=True):
                if relation_type and edge_data.get('relation') != relation_type:
                    continue
                if source not in visited: # source is the neighbor here
                    visited.add(source)
                    queue.append((source, depth + 1))
                    related_entities.append(source)

        return list(set(related_entities))


    def visualize_graph(self, entities: List[str] = None, figsize: Tuple[int, int] = (15, 12)):
        if entities:
            expanded_entities = set(entities)
            for entity in entities:
                if entity in self.graph:
                    for neighbor in nx.neighbors(self.graph, entity): 
                        expanded_entities.add(neighbor)
            g = self.graph.subgraph(list(expanded_entities)).copy()
            # Remove isolated nodes to make visualization more focused on relationships
            g.remove_nodes_from(list(nx.isolates(g)))
        else:
            if len(self.graph.nodes()) > 50:
                nodes = list(self.graph.nodes())
                sampled_nodes = np.random.choice(nodes, size=50, replace=False)
                g = self.get_subgraph(sampled_nodes)
                g.remove_nodes_from(list(nx.isolates(g)))
            else:
                g = self.graph.copy()

        if not g.nodes():
            print("å­å›¾ä¸ºç©ºï¼Œæ— æ³•å¯è§†åŒ–ã€‚")
            return

        plt.figure(figsize=figsize)
        try:
            pos = nx.kamada_kawai_layout(g)
        except Exception: 
             pos = nx.spring_layout(g, k=0.15, iterations=20) 

        nx.draw_networkx_nodes(g, pos, node_size=300, alpha=0.7, node_color='skyblue')
        nx.draw_networkx_edges(g, pos, width=0.8, alpha=0.3, edge_color='gray', arrows=True, arrowstyle='->', arrowsize=10)
        nx.draw_networkx_labels(g, pos, font_size=9, font_family='SimHei')

        edge_labels_dict = {}
        for u, v, data in g.edges(data=True):
            if (u,v) not in edge_labels_dict: 
                 edge_labels_dict[(u,v)] = data.get('relation', '')

        nx.draw_networkx_edge_labels(g, pos, edge_labels=edge_labels_dict, font_size=7, font_family='SimHei', alpha=0.8)

        plt.title("ä¸­åŒ»çŸ¥è¯†å­å›¾å¯è§†åŒ–", fontproperties={'family':'SimHei', 'size':16})
        plt.axis('off')
        plt.tight_layout()
        plt.show()


class GraphRAG:
    def __init__(self, knowledge_graph: TCMKnowledgeGraph):
        self.kg = knowledge_graph
        self.llm_cache = {}

    def query_moonshot_api(self, prompt: str, temperature: float = 0.3, max_tokens: int = 2048) -> str: 
        cache_key = f"{prompt}_{temperature}_{max_tokens}"
        if cache_key in self.llm_cache:
            # print("LLMä»ç¼“å­˜åŠ è½½")
            return self.llm_cache[cache_key]

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {MOONSHOT_API_KEY}"
        }
        data = {
            "model": "moonshot-v1-32k", 
            "messages": [{"role": "user", "content": prompt}],
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        try:
            response = requests.post(MOONSHOT_API_URL, headers=headers, json=data, timeout=120) 
            response.raise_for_status()
            result = response.json()
            response_text = result["choices"][0]["message"]["content"]
            self.llm_cache[cache_key] = response_text
            return response_text
        except requests.exceptions.Timeout:
            print("è°ƒç”¨APIè¶…æ—¶ã€‚")
            return "è°ƒç”¨APIè¶…æ—¶ï¼Œè¯·ç¨åå†è¯•æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚"
        except Exception as e:
            print(f"è°ƒç”¨APIå¤±è´¥: {e}")
            return f"è°ƒç”¨APIæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"

    
    def extract_keywords_and_intent(self, query: str) -> Dict[str, Any]:
        """
        ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯å’Œç”¨æˆ·æ„å›¾ï¼ˆä¾‹å¦‚ï¼Œæ˜¯æŸ¥æ‰¾ä¿¡æ¯è¿˜æ˜¯å¯»æ±‚æ²»ç–—æ–¹æ¡ˆï¼‰ã€‚
        """
        prompt = f"""
        è¯·åˆ†æä»¥ä¸‹ä¸­åŒ»ç›¸å…³æŸ¥è¯¢ï¼Œæå–å…³é”®å®ä½“è¯ï¼ˆå¦‚ä¸­è¯åã€ç—‡çŠ¶ã€ç–¾ç—…åã€æ–¹å‰‚åç­‰ï¼‰ï¼Œå¹¶åˆ¤æ–­ç”¨æˆ·çš„ä¸»è¦æ„å›¾ã€‚
        ä¸»è¦æ„å›¾å¯ä»¥æ˜¯ï¼š"æŸ¥è¯¢å®ä½“ä¿¡æ¯"ã€"å¯»æ±‚æ²»ç–—æ–¹æ¡ˆ"ã€"æ¯”è¾ƒå®ä½“"ã€"æœªçŸ¥"ã€‚
        è¯·æŒ‰JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å« "keywords" (å­—ç¬¦ä¸²åˆ—è¡¨) å’Œ "intent" (å­—ç¬¦ä¸²) ä¸¤ä¸ªå­—æ®µã€‚

        æŸ¥è¯¢: "{query}"

        JSONè¾“å‡º:
        """
        response_text = self.query_moonshot_api(prompt, temperature=0.1, max_tokens=512)
        try:
            clean_response = re.sub(r"```json\n?|\n?```", "", response_text).strip()
            result = json.loads(clean_response)
            if not isinstance(result.get("keywords"), list): 
                result["keywords"] = []
            if not isinstance(result.get("intent"), str):
                 result["intent"] = "æœªçŸ¥"
            return result
        except json.JSONDecodeError:
            print(f"å…³é”®è¯å’Œæ„å›¾æå–JSONè§£æå¤±è´¥: {response_text}")
            keywords = [kw.strip() for kw in response_text.splitlines() if kw.strip() and not kw.startswith("{")] 
            return {"keywords": keywords if keywords else [query], "intent": "æœªçŸ¥"}

    def retrieve_relevant_knowledge(self, query: str, extracted_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        keywords = extracted_info.get("keywords", [])
        intent = extracted_info.get("intent", "æœªçŸ¥")
        # print(f"æå–çš„å…³é”®è¯: {keywords}, ç”¨æˆ·æ„å›¾: {intent}")

        relevant_knowledge_items = []
        processed_entities_for_detail = set() 

        all_matched_entities = []
        for keyword in keywords:
            matched = self.kg.search_by_keyword(keyword)
            all_matched_entities.extend(matched)
        unique_entities = list(set(all_matched_entities))
        # print(f"çŸ¥è¯†å›¾è°±ä¸­åŒ¹é…åˆ°çš„å®ä½“: {unique_entities}")

        candidate_plans_for_details = []

        # 1. If the intention is to "seek treatment options" or if the query contains treatment-related vocabulary
        is_seeking_treatment_intent = (intent == "å¯»æ±‚æ²»ç–—æ–¹æ¡ˆ") or \
                                    any(treat_kw in query.lower() for treat_kw in ["æ€ä¹ˆåŠ", "å¦‚ä½•æ²»", "ç”¨ä»€ä¹ˆè¯", "æ²»ç–—æ–¹æ³•", "æ–¹å‰‚"])

        if is_seeking_treatment_intent:
            # a. Directly matching treatment plans by name
            for entity_name in unique_entities:
                if "æ²»ç–—æ–¹æ¡ˆ" in entity_name and entity_name not in processed_entities_for_detail:
                    candidate_plans_for_details.append(entity_name)
                    processed_entities_for_detail.add(entity_name)
            
            # b. Reverse search or forward search for treatment plans through keywords (symptoms/diseases)
            symptom_disease_keywords = [kw for kw in keywords if "æ²»ç–—æ–¹æ¡ˆ" not in kw] 
            
            for keyword_sd in symptom_disease_keywords:
                for s, p, o, book, chap, source_str in self.kg.triples_with_source:
                    if o.lower() == keyword_sd.lower() and \
                    ("æ²»ç–—ç–¾ç—…" in p or "æ²»ç–—ç—‡çŠ¶" in p or "å…·æœ‰ç—‡çŠ¶" in p):
                        if s not in processed_entities_for_detail:
                            candidate_plans_for_details.append(s)
                            processed_entities_for_detail.add(s)
                    elif s.lower() == keyword_sd.lower() and \
                        ("æ¨èæ–¹å‰‚" in p or "å®šä¹‰æ²»ç–—æ–¹æ¡ˆ" in p):
                        if o not in processed_entities_for_detail:
                            candidate_plans_for_details.append(o)
                            processed_entities_for_detail.add(o)
            
            candidate_plans_for_details = list(set(candidate_plans_for_details))
            # print(f"åˆæ­¥å€™é€‰æ²»ç–—æ–¹æ¡ˆåˆ—è¡¨ (æ„å›¾å¯¼å‘): {candidate_plans_for_details}")


        # 2. Obtain detailed information on the identified treatment plan
        MAX_PLANS_TO_DETAIL = 5
        detailed_plans_count = 0
        for plan_name in candidate_plans_for_details:
            if detailed_plans_count >= MAX_PLANS_TO_DETAIL: break
            # print(f"æ­£åœ¨ä¸ºæ²»ç–—æ–¹æ¡ˆ '{plan_name}' æ”¶é›†è¯¦ç»†ä¿¡æ¯...")
            plan_details = self.kg.get_treatment_plan_full_details(plan_name)
            if plan_details.get("ç»„æˆ") or plan_details.get("åŠŸèƒ½ä¸»æ²»"):
                relevant_knowledge_items.append({"type": "æ²»ç–—æ–¹æ¡ˆ", "name": plan_name, "details": plan_details})
                detailed_plans_count += 1
                processed_entities_for_detail.add(plan_name) 

        # 3. Obtain basic triplets related to unique_dentities (schemes for which details have not been obtained) as supplementary information
        if not relevant_knowledge_items or len(relevant_knowledge_items) < MAX_PLANS_TO_DETAIL + 1 or not is_seeking_treatment_intent:
            basic_triples_context_with_source = []
            MAX_TRIPLES_PER_ENTITY = 2
            TOTAL_MAX_BASIC_TRIPLES = 5 
            triples_collected_count = 0

            for entity in unique_entities:
                if entity in processed_entities_for_detail:
                    continue
                if triples_collected_count >= TOTAL_MAX_BASIC_TRIPLES: break

                relations = self.kg.get_entity_relations(entity)
                sorted_relations = self._rank_triples_by_relevance(relations, query, keywords)
                
                added_count_for_entity = 0
                for triple_with_source in sorted_relations:
                    if triples_collected_count >= TOTAL_MAX_BASIC_TRIPLES or added_count_for_entity >= MAX_TRIPLES_PER_ENTITY:
                        break
                    basic_triples_context_with_source.append(triple_with_source)
                    triples_collected_count +=1
                    added_count_for_entity +=1
            
            if basic_triples_context_with_source:
                has_triple_list = any(item.get("type") == "ä¸‰å…ƒç»„åˆ—è¡¨" for item in relevant_knowledge_items)
                if not has_triple_list:
                    relevant_knowledge_items.append({"type": "ä¸‰å…ƒç»„åˆ—è¡¨", 
                                                    "triples": basic_triples_context_with_source})

        MAX_KNOWLEDGE_ITEMS = 3
        return relevant_knowledge_items[:MAX_KNOWLEDGE_ITEMS]


    def _rank_triples_by_relevance(self, triples_with_source: List[Tuple[str, str, str, str]], 
                                   query: str, keywords: List[str]) -> List[Tuple[str, str, str, str]]:
            scored_triples = []
            for triple_item in triples_with_source:
                s, p, o, _ = triple_item
                score = 0
                for kw in keywords:
                    kw_lower = kw.lower()
                    if kw_lower in s.lower(): score += 5
                    if kw_lower in o.lower(): score += 3
                    if kw_lower in p.lower(): score += 1
                for word in query.split():
                    word_lower = word.lower()
                    if word_lower not in [k.lower() for k in keywords]:
                        if word_lower in s.lower(): score += 2
                        if word_lower in o.lower(): score += 1
                scored_triples.append((triple_item, score))
            return [t[0] for t in sorted(scored_triples, key=lambda x: x[1], reverse=True)]

    def _format_knowledge_for_llm(self, knowledge_items: List[Dict[str, Any]]) -> str:
        formatted_text = ""
        if not knowledge_items:
            return "æœªä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°ä¸æŸ¥è¯¢ç›´æ¥ç›¸å…³çš„è¯¦ç»†ä¸­åŒ»çŸ¥è¯†ã€‚" 

        plan_count = 0
        for item in knowledge_items:
            if item["type"] == "æ²»ç–—æ–¹æ¡ˆ":
                plan_count += 1
                details = item["details"]
                formatted_text += f"ã€æ²»ç–—æ–¹æ¡ˆ {plan_count}: {details.get('åç§°', item['name'])}ã€‘\n" 

                plan_sources = details.get("æ¥æºä¿¡æ¯", [])
                if plan_sources:
                    source_display = plan_sources[0] if len(plan_sources) == 1 else '; '.join(plan_sources)
                    formatted_text += f"  ä¸»è¦æ¥æº: {source_display}\n"

                if details.get("åŠŸèƒ½ä¸»æ²»"):
                    formatted_text += f"  åŠŸèƒ½ä¸»æ²»:\n"
                    for val_src_item in details["åŠŸèƒ½ä¸»æ²»"]:
                        source_tag = f"(æ¥æº: {val_src_item['source']})" if val_src_item['source'] != "æœªçŸ¥æ¥æº" else ""
                        formatted_text += f"    - {val_src_item['value']} {source_tag}\n".strip() + "\n"
                if details.get("ç›¸å…³ç—‡çŠ¶"):
                    formatted_text += f"  ç›¸å…³ç—‡çŠ¶:\n"
                    for val_src_item in details["ç›¸å…³ç—‡çŠ¶"]:
                        source_tag = f"(æ¥æº: {val_src_item['source']})" if val_src_item['source'] != "æœªçŸ¥æ¥æº" else ""
                        formatted_text += f"    - {val_src_item['value']} {source_tag}\n".strip() + "\n"
                if details.get("ç»„æˆ"):
                    formatted_text += "  ç»„æˆ:\n"
                    for comp in details["ç»„æˆ"]:
                        dose_str = comp.get('å‰‚é‡', 'æœªçŸ¥')
                        dose_source_str = ""
                        if comp.get("å‰‚é‡æ¥æº"):
                            unique_dose_sources = sorted(list(set(comp['å‰‚é‡æ¥æº']))) 
                            if unique_dose_sources and unique_dose_sources != ["æœªçŸ¥æ¥æº"]:
                                dose_source_str = f" (å‰‚é‡æ¥æº: {'; '.join(unique_dose_sources)})"
                        
                        herb_source_str = f"(è¯ææ¡ç›®æ¥æº: {comp['æ¥æº']})" if comp['æ¥æº'] != "æœªçŸ¥æ¥æº" else ""
                        formatted_text += f"    - {comp['è¯æ']} (å‰‚é‡: {dose_str}{dose_source_str}) {herb_source_str}\n".strip() + "\n"
                
                for detail_key, display_name in [("åˆ¶å¤‡æ–¹æ³•", "åˆ¶å¤‡æ–¹æ³•"), ("å¤‡æ³¨", "å¤‡æ³¨")]:
                    if details.get(detail_key):
                        formatted_text += f"  {display_name}:\n"
                        for val_src_item in details[detail_key]:
                            source_tag = f"(æ¥æº: {val_src_item['source']})" if val_src_item['source'] != "æœªçŸ¥æ¥æº" else ""
                            formatted_text += f"    - {val_src_item['value']} {source_tag}\n".strip() + "\n"
                formatted_text += "\n"

            elif item["type"] == "ä¸‰å…ƒç»„åˆ—è¡¨" and item.get("triples"):
                formatted_text += "ã€å…¶ä»–ç›¸å…³çŸ¥è¯†ç‚¹ã€‘:\n"
                for s, p, o, source_str in item["triples"]:
                    source_tag = f"(æ¥æº: {source_str})" if source_str != "æœªçŸ¥æ¥æº" else ""
                    formatted_text += f"  - â€œ{s}â€ {p} â€œ{o}â€ {source_tag}\n".strip() + "\n"
                formatted_text += "\n"
        
        if not formatted_text.strip(): 
            return "æœªä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°è¶³å¤Ÿè¯¦ç»†çš„ä¸­åŒ»çŸ¥è¯†æ¥å›ç­”è¯¥é—®é¢˜ã€‚"
        return formatted_text.strip()

    def generate_graphrag_response_only(self, query: str, include_context_debug: bool = False) -> Tuple[str, str, List[Dict[str, Any]], Dict[str,Any]]:
        extracted_info = self.extract_keywords_and_intent(query)
        relevant_knowledge_items = self.retrieve_relevant_knowledge(query, extracted_info)
        context_for_llm = self._format_knowledge_for_llm(relevant_knowledge_items) 
        intent = extracted_info.get("intent", "æœªçŸ¥")

        no_kg_context_messages = [
            "æœªä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°ä¸æŸ¥è¯¢ç›´æ¥ç›¸å…³çš„è¯¦ç»†ä¸­åŒ»çŸ¥è¯†ã€‚",
            "æœªä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°è¶³å¤Ÿè¯¦ç»†çš„ä¸­åŒ»çŸ¥è¯†æ¥å›ç­”è¯¥é—®é¢˜ã€‚"
        ]
        if context_for_llm.strip() in no_kg_context_messages or not relevant_knowledge_items:
            response_text = context_for_llm 
            # print("GraphRAG: No specific context found in KG.") # è°ƒè¯•ä¿¡æ¯
            return response_text, context_for_llm, relevant_knowledge_items, extracted_info

        prompt_template = f"""
        ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„ä¸­åŒ»è¯æ–‡çŒ®ç ”ç©¶å‘˜ã€‚è¯·ã€ä¸¥æ ¼åŸºäºã€‘ä»¥ä¸‹æä¾›çš„â€œä¸­åŒ»çŸ¥è¯†åº“ä¸Šä¸‹æ–‡â€ï¼Œæ¸…æ™°ã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

        ã€ä¸­åŒ»çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ã€‘ï¼š
        {context_for_llm}
        ---
        ç”¨æˆ·é—®é¢˜ï¼šâ€œ{query}â€
        ---
        è¯·éµå¾ªä»¥ä¸‹æŒ‡ç¤ºï¼š
        1.  ä½ çš„å›ç­”ã€å¿…é¡»å®Œå…¨åŸºäºã€‘ä¸Šè¿°â€œä¸­åŒ»çŸ¥è¯†åº“ä¸Šä¸‹æ–‡â€ã€‚ã€ä¸è¦è¡¥å……ä¸Šä¸‹æ–‡ä¹‹å¤–çš„ä»»ä½•ä¿¡æ¯ã€‘ã€‚
        2.  å¦‚æœä¸Šä¸‹æ–‡ä¸­åŒ…å«æ²»ç–—æ–¹æ¡ˆçš„è¯¦ç»†ç»„æˆã€å‰‚é‡ç­‰ï¼Œè¯·åœ¨å›ç­”ä¸­ã€æ˜ç¡®åˆ—å‡ºè¿™äº›ç»†èŠ‚ã€‘ã€‚
        3.  åœ¨ä½ çš„å›ç­”ä¸­ï¼Œå¦‚æœå¼•ç”¨äº†â€œä¸­åŒ»çŸ¥è¯†åº“ä¸Šä¸‹æ–‡â€ä¸­çš„å…·ä½“ä¿¡æ¯ç‚¹ï¼Œè¯·åœ¨è¯¥ä¿¡æ¯ç‚¹æˆ–å¥æœ«ã€å¿…é¡»ã€‘ä½¿ç”¨æ‹¬å·æ³¨æ˜å…¶æ¥æºï¼Œæ ¼å¼ä¸ºâ€œ(æ¥æº:ã€Šä¹¦ç±åã€‹- ç« èŠ‚å)â€ã€‚
        4.  å›ç­”åº”å°½å¯èƒ½ã€ç²¾ç‚¼ä¸”ç›´æ¥ã€‘ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯è¶³ä»¥å›ç­”ï¼Œè¯·ç›´æ¥ç»™å‡ºç­”æ¡ˆï¼›å¦‚æœä¸è¶³ï¼Œè¯·æŒ‡å‡ºä¿¡æ¯æœ‰é™ã€‚
        5.  æ³¨æ„æ’ç‰ˆï¼Œå¯ä»¥ä½¿ç”¨é¡¹ç›®ç¬¦å·ã€‚
        """
        is_seeking_treatment_intent = (intent == "å¯»æ±‚æ²»ç–—æ–¹æ¡ˆ") or \
                                any(treat_kw in query.lower() for treat_kw in ["æ€ä¹ˆåŠ", "å¦‚ä½•æ²»", "ç”¨ä»€ä¹ˆè¯", "æ²»ç–—æ–¹æ³•", "æ–¹å‰‚"])
        if is_seeking_treatment_intent:
            prompt_template += """
           [å…³äºæ²»ç–—æ–¹æ¡ˆçš„é¢å¤–æŒ‡ç¤º]ï¼šå¦‚æœä¸Šä¸‹æ–‡ä¸­æåˆ°äº†ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„æ²»ç–—æ–¹æ¡ˆï¼Œè¯·è¯¦ç»†è¯´æ˜å…¶ã€åç§°ã€ä¸»è¦åŠŸèƒ½ä¸»æ²»ã€ç»„æˆè¯æï¼ˆåŠå‰‚é‡ï¼Œè‹¥æœ‰ï¼‰ã€åˆ¶å¤‡æ–¹æ³•/å¤‡æ³¨ï¼ˆè‹¥æœ‰ï¼‰ã€‘ï¼Œå¹¶åŠ¡å¿…æ ‡æ³¨å„é¡¹ä¿¡æ¯çš„æ¥æºã€‚
           """
        prompt_template += "\nè¯·ç»™å‡ºä½ çš„å›ç­”ï¼š"

        # print(f"--- Prompt for GraphRAG Response Only (Temperature: 0.1) ---") # è°ƒè¯•
        # print(prompt_template) # è°ƒè¯•
        # print(f"-------------------------------------------------------------") # è°ƒè¯•

        response_text = self.query_moonshot_api(prompt_template, temperature=0.1, max_tokens=2000)

        if include_context_debug:
            debug_output = (
                f"---DEBUG: GraphRAG Context Generation---\n"
                f"æ„å›¾ï¼š{intent}\nå…³é”®è¯ï¼š{extracted_info.get('keywords')}\n"
                f"æ£€ç´¢åˆ°çš„çŸ¥è¯†é¡¹æ•°é‡ï¼š{len(relevant_knowledge_items)}\n"
                f"---CONTEXT FOR GraphRAG-Kimi---\n{context_for_llm}\n"
                f"---GraphRAG-Kimi RESPONSE---\n{response_text}"
            )
            # print(debug_output) 
        
        return response_text, context_for_llm, relevant_knowledge_items, extracted_info        

    def get_general_kimi_response(self, query: str, temperature: float = 0.5, max_tokens: int = 2048) -> str:
        prompt = f"""
        ä½ æ˜¯ä¸€ä½çŸ¥è¯†æ¸Šåšä¸”èµ„æ·±çš„ä¸­åŒ»ä¸“å®¶ã€‚è¯·é’ˆå¯¹ä»¥ä¸‹ç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼Œæä¾›ä¸€ä¸ªå…¨é¢ã€è¯¦ç»†ã€ä¸”ç»“æ„æ¸…æ™°çš„è§£ç­”ã€‚
        è¯·è·Ÿæ®å…·ä½“é—®é¢˜ï¼ˆè¯ç‰©/ç—…ç—‡æŸ¥è¯¢ã€åŠŸèƒ½/ä½œç”¨ç­‰ï¼‰å°½å¯èƒ½ä»ä¸åŒæ–¹é¢ï¼ˆä¾‹å¦‚ï¼šå®šä¹‰ã€ç—…å› ç—…æœºã€ä¸»è¦ç±»å‹ã€å¸¸è§ç—‡çŠ¶ã€è¯Šæ–­è¦ç‚¹ã€æ²»ç–—åŸåˆ™ã€å¸¸ç”¨æ–¹è¯ä¸¾ä¾‹ã€é¢„åè½¬å½’ã€ç”Ÿæ´»è°ƒç†åŠæ³¨æ„äº‹é¡¹ç­‰ï¼Œæ ¹æ®é—®é¢˜ç±»å‹é…Œæƒ…é€‰æ‹©ï¼‰ä¸”æœ‰é‡ç‚¹åœ°è¿›è¡Œé˜è¿°ã€‚
        è¯·ç¡®ä¿è¯­è¨€ä¸“ä¸šã€ä¸¥è°¨ï¼ŒåŒæ—¶ä¹Ÿè¦æ˜“äºç†è§£ã€‚
        
        ç”¨æˆ·é—®é¢˜ï¼š â€œ{query}â€

        ä½ çš„ä¸“ä¸šè§£ç­”ï¼š
        """
        # print("--- Prompt for General Kimi Response ---") # è°ƒè¯•ç”¨
        # print(prompt)
        # print("--------------------------------------")
        return self.query_moonshot_api(prompt, temperature=temperature, max_tokens=max_tokens)
    
    def synthesize_responses(self, query: str, graphrag_response: str, general_response: str,
                             temperature: float = 0.3, max_tokens: int = 3000) -> str:

        is_graphrag_valid = not any(
            msg in graphrag_response for msg in [
                "æœªä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°ä¸æŸ¥è¯¢ç›´æ¥ç›¸å…³çš„è¯¦ç»†ä¸­åŒ»çŸ¥è¯†ã€‚",
                "æœªä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°è¶³å¤Ÿè¯¦ç»†çš„ä¸­åŒ»çŸ¥è¯†æ¥å›ç­”è¯¥é—®é¢˜ã€‚"
            ]
        ) and graphrag_response.strip()

        synthesis_prompt = f"""
        ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ä¸­åŒ»å†…å®¹ç¼–è¾‘å’Œæ’°ç¨¿ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ä»¥ä¸‹ä¸¤ä»½å…³äºç”¨æˆ·é—®é¢˜â€œ{query}â€çš„å›ç­”ï¼Œå·§å¦™åœ°èåˆæˆä¸€ä¸ªå•ä¸€ã€å…¨é¢ã€å‡†ç¡®ã€è¡Œæ–‡æµç•…è‡ªç„¶ã€ä¸”å¸¦æœ‰æ¸…æ™°æ¥æºæ ‡æ³¨çš„æœ€ç»ˆä¸“ä¸šç­”æ¡ˆã€‚

        ã€ç¬¬ä¸€ä»½å›ç­”ã€‘ï¼ˆæ­¤å›ç­”ä¸»è¦åŸºäºä¸€ä¸ªä¸“é—¨çš„å¤ç±ä¸­åŒ»çŸ¥è¯†å›¾è°±æ£€ç´¢ï¼Œå¹¶ç”±AIåˆæ­¥æ€»ç»“ï¼Œå…¶æ ¸å¿ƒä»·å€¼åœ¨äºåŒ…å«å…·ä½“çš„å¤ç±æ–‡çŒ®æ¥æºä¿¡æ¯å’Œç»†èŠ‚ï¼‰ï¼š
        ---
        {graphrag_response if is_graphrag_valid else "ï¼ˆçŸ¥è¯†å›¾è°±ä¸­æœªæ‰¾åˆ°ä¸é—®é¢˜ç›´æ¥ç›¸å…³çš„å…·ä½“è®°è½½ï¼‰"}
        ---

        ã€ç¬¬äºŒä»½å›ç­”ã€‘ï¼ˆæ­¤å›ç­”æ¥è‡ªä¸€ä¸ªé€šç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç‰¹ç‚¹æ˜¯çŸ¥è¯†é¢è¾ƒå¹¿ï¼Œè§£é‡Šå¯èƒ½æ›´ç³»ç»Ÿå’Œè¯¦å°½ï¼Œä½†é€šå¸¸ä¸åŒ…å«å…·ä½“çš„å¤ç±æ–‡çŒ®æ¥æºï¼‰ï¼š
        ---
        {general_response}
        ---

        è¯·éµå¾ªä»¥ä¸‹ã€ä¸¥æ ¼çš„æ•´åˆæŒ‡ç¤ºã€‘è¿›è¡Œæ“ä½œï¼š

        1.  **ç»“æ„ä¸å†…å®¹ä¸»å¹²**ï¼šä»¥ã€ç¬¬äºŒä»½å›ç­”ã€‘ï¼ˆé€šç”¨å›ç­”ï¼‰çš„ç»“æ„ã€å¹¿åº¦å’Œç³»ç»Ÿæ€§è®ºè¿°ä½œä¸ºæ•´åˆåç­”æ¡ˆçš„åŸºç¡€æ¡†æ¶å’Œä¸»è¦å™è¿°æµç¨‹ã€‚
        2.  **ç»†èŠ‚èå…¥ä¸æ·±åº¦å¢å¼º**ï¼š
            * å°†ã€ç¬¬ä¸€ä»½å›ç­”ã€‘ï¼ˆå›¾è°±ç‰ˆå›ç­”ï¼‰ä¸­çš„ã€æ‰€æœ‰å…·ä½“çš„ã€æœ‰ä»·å€¼çš„ä¿¡æ¯ç‚¹ï¼Œå¦‚è¯ç‰©ç»„æˆã€å‰‚é‡ã€ç‰¹å®šçš„æ²»ç–—æ–¹æ³•ç»†èŠ‚ã€å¤ç±ä¸­çš„ç‹¬ç‰¹è®ºè¿°ï¼Œä»¥åŠæœ€é‡è¦çš„â€”â€”æ–‡çŒ®æ¥æºæ ‡æ³¨ã€‘ï¼ˆä¾‹å¦‚â€œ(æ¥æº: ã€Šé‡‘åŒ®è¦ç•¥ã€‹ - æŸæŸç¯‡)â€ï¼‰â€”â€”ã€å‡†ç¡®æ— è¯¯ä¸”æä¸ºè‡ªç„¶åœ°ã€‘èå…¥åˆ°ã€ç¬¬äºŒä»½å›ç­”ã€‘çš„ç›¸åº”è®ºè¿°æ®µè½æˆ–çŸ¥è¯†ç‚¹ä¸­ã€‚
            * **æ ¸å¿ƒç›®æ ‡**ï¼šè®©è¿™äº›æ¥è‡ªå¤ç±çš„ä¿¡æ¯çœ‹èµ·æ¥åƒæ˜¯å¯¹é€šç”¨å›ç­”ä¸­ç›¸åº”è®ºç‚¹çš„**åŸç”Ÿè¡¥å……ã€å…·ä½“ä¾‹è¯ã€æˆ–æ·±åŒ–è§£é‡Š**ï¼Œè€Œä¸æ˜¯ç”Ÿç¡¬çš„æ’å…¥æˆ–ç‹¬ç«‹çš„é™„åŠ ä¿¡æ¯å—ã€‚åŠ›æ±‚ä½¿æ•´åˆåçš„æ–‡æœ¬æµ‘ç„¶ä¸€ä½“ã€‚
        3.  **æ¥æºä¿¡æ¯çš„å¤„ç†ä¸å‘ˆç°**ï¼š
            * å¦‚æœä¸¤ä»½å›ç­”ä¸­å­˜åœ¨ä¿¡æ¯é‡å ï¼ˆä¾‹å¦‚ï¼Œå¯¹åŒä¸€åŠŸæ•ˆæˆ–æ–¹å‰‚çš„æè¿°ï¼‰ï¼Œã€ä¼˜å…ˆé‡‡ç”¨å¹¶æ•´åˆæ¥è‡ªç¬¬ä¸€ä»½å›ç­”ä¸­å¸¦æœ‰æ–‡çŒ®æ¥æºçš„è¡¨è¿°å’Œç»†èŠ‚ã€‘ï¼Œç”¨å…¶æ¥ä¸°å¯Œã€å…·ä½“åŒ–æˆ–æ›¿ä»£é€šç”¨å›ç­”ä¸­çš„å¯¹åº”å†…å®¹ã€‚
            * æ‰€æœ‰æ¥è‡ªç¬¬ä¸€ä»½å›ç­”çš„æ–‡çŒ®æ¥æºæ ‡æ³¨éƒ½å¿…é¡»ä¿ç•™ï¼Œå¹¶æ¸…æ™°åœ°é™„åœ¨ç›¸åº”çš„ä¿¡æ¯ç‚¹ä¹‹åã€‚ç¡®ä¿æ¥æºæ ¼å¼ä¸ºâ€œ(æ¥æº: ã€Šä¹¦ç±åã€‹ - ç« èŠ‚å)â€ã€‚æ¥æºæ ‡æ³¨åº”ä½œä¸ºä¿¡æ¯ç‚¹é˜è¿°å®Œæ¯•åçš„è‡ªç„¶æ”¶å°¾ï¼Œé¿å…çªå…€æ„Ÿã€‚
        4.  **å¤„ç†å›¾è°±æ— ç‰¹å®šä¿¡æ¯çš„æƒ…å†µ**ï¼šå¦‚æœã€ç¬¬ä¸€ä»½å›ç­”ã€‘æ˜ç¡®æŒ‡å‡ºâ€œæœªæ‰¾åˆ°ä¿¡æ¯â€æˆ–å†…å®¹ä¸ºç©ºï¼Œåˆ™æœ€ç»ˆç­”æ¡ˆä¸»è¦ä¾èµ–ã€ç¬¬äºŒä»½å›ç­”ã€‘ã€‚æ­¤æ—¶ï¼Œæ— éœ€åˆ»æ„æåŠçŸ¥è¯†å›¾è°±æœªæ‰¾åˆ°ä¿¡æ¯ï¼Œé™¤éä½ è®¤ä¸ºè¿™æ ·çš„è¯´æ˜å¯¹ç”¨æˆ·æœ‰ç›Šã€‚
        5.  **å†…å®¹è¡¥å……ä¸æ–‡è„‰æ‰©å±•**ï¼š
            * å¦‚æœã€ç¬¬ä¸€ä»½å›ç­”ã€‘æä¾›äº†ã€ç¬¬äºŒä»½å›ç­”ã€‘ä¸­å®Œå…¨æ²¡æœ‰æåŠçš„ã€ä¸é—®é¢˜ç›¸å…³çš„å…·ä½“æ²»ç–—æ–¹æ¡ˆã€è¯ç‰©ç»„æˆã€å‰‚é‡ã€å¤ç±è§‚ç‚¹ç­‰é‡è¦ç»†èŠ‚ã€‘ï¼Œè¯·åŠ¡å¿…å°†è¿™äº›æœ‰ä»·å€¼çš„å†…å®¹ã€æ— ç¼åœ°ã€åˆä¹é€»è¾‘åœ°æ•´åˆã€‘åˆ°æœ€ç»ˆç­”æ¡ˆçš„æ°å½“éƒ¨åˆ†ã€‚
            * è¿™å¯èƒ½éœ€è¦ä½ å¯¹ã€ç¬¬äºŒä»½å›ç­”ã€‘çš„å±€éƒ¨ç»“æ„è¿›è¡Œå¾®è°ƒæˆ–æ‰©å±•ï¼Œä»¥ç¡®ä¿æ–°å¢ä¿¡æ¯çš„èå…¥æ—¢è‡ªç„¶åˆä¿æŒäº†æ•´ä½“è®ºè¿°çš„è¿è´¯æ€§å’Œæµç•…æ€§ã€‚
        6.  **ä¸“ä¸šæ€§ã€å¯è¯»æ€§ä¸æ–‡é£ç»Ÿä¸€**ï¼š
            * æœ€ç»ˆç­”æ¡ˆåº”ä¿æŒä¸­åŒ»çš„ä¸“ä¸šæœ¯è¯­å‡†ç¡®æ€§ï¼ŒåŒæ—¶è¯­è¨€è¡¨è¾¾åº”æµç•…è‡ªç„¶ã€ç»“æ„æ¸…æ™°ï¼Œæ˜“äºæ™®é€šç”¨æˆ·ç†è§£ï¼ˆå¯é€‚å½“ä½¿ç”¨é¡¹ç›®ç¬¦å·ã€åˆ†ç‚¹é˜è¿°ç­‰æ–¹å¼ä¼˜åŒ–æ’ç‰ˆï¼‰ã€‚
            * **è‡³å…³é‡è¦çš„æ˜¯**ï¼šç¡®ä¿æ•´åˆåçš„å…¨æ–‡æ–‡é£ç»Ÿä¸€ã€è¯­è°ƒä¸€è‡´ï¼Œé¿å…å‡ºç°ä¸¤ç§å›ç­”é£æ ¼çš„æ˜æ˜¾å‰²è£‚æ„Ÿï¼Œè®©è¯»è€…æ„Ÿè§‰è¿™æ˜¯ç”±ä¸€ä½ä¸“å®¶ä¸€æ°”å‘µæˆæ’°å†™çš„å†…å®¹ã€‚åŒæ—¶ï¼Œå¯¹äºè¯»è€…é—®é¢˜ä¸­çš„å…³æ³¨ç‚¹ï¼ˆæŸç§è¯ç‰©/ç—…ç—‡çš„æŸ¥è¯¢ã€åŠŸèƒ½ã€ä½œç”¨ã€å®šä¹‰ç­‰ï¼‰åº”åœ¨çªå‡ºç›¸åº”çš„é‡ç‚¹ã€‚
        7.  **æ ‡å‡†æé†’**ï¼šåœ¨æœ€ç»ˆç­”æ¡ˆçš„æœ«å°¾ï¼ŒåŠ¡å¿…åŠ ä¸Šæ ‡å‡†çš„ç”¨è¯æé†’ï¼šâ€œè¯·æ³¨æ„ï¼Œä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œå…·ä½“ç”¨è¯å’Œæ²»ç–—æ–¹æ¡ˆè¯·åŠ¡å¿…å’¨è¯¢ä¸“ä¸šä¸­åŒ»å¸ˆè¿›è¡Œè¾¨è¯è®ºæ²»ï¼Œåˆ‡å‹¿è‡ªè¡Œç”¨è¯ã€‚â€

        è¯·åŸºäºä»¥ä¸Šæ‰€æœ‰æŒ‡ç¤ºï¼Œä»¥ä¸“ä¸šçš„åˆ¤æ–­å’Œé«˜è¶…çš„ç¼–è¾‘æŠ€å·§ï¼Œè¾“å‡ºæ•´åˆåçš„ã€æœ€ç»ˆä¸“ä¸šç­”æ¡ˆã€‘ï¼š
        """
        # print("--- Prompt for Synthesis ---") # è°ƒè¯•ç”¨
        # print(synthesis_prompt)
        # print("----------------------------")
        return self.query_moonshot_api(synthesis_prompt, temperature=temperature, max_tokens=max_tokens)

class TCMGraphRAGApp:
    def __init__(self, csv_path: str):
        if not MOONSHOT_API_KEY:
            print("è­¦å‘Š: æœªè®¾ç½®MOONSHOT_API_KEYç¯å¢ƒå˜é‡ï¼ŒAPIè°ƒç”¨å°†å¤±è´¥")
        print("æ­£åœ¨åŠ è½½ä¸­åŒ»çŸ¥è¯†å›¾è°±...")
        self.kg = TCMKnowledgeGraph(csv_path)
        self.rag = GraphRAG(self.kg)
        print("ä¸­åŒ»çŸ¥è¯†æ£€ç´¢ä¸é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")

    def run_interactive_cli(self):
        print("=" * 80)
        print(" ä¸­åŒ»æ™ºèƒ½é—®ç­”ç³»ç»Ÿ (è¾“å…¥'é€€å‡º'ç»“æŸå¯¹è¯, è¾“å…¥ 'å¯è§†åŒ–:æ‚¨çš„é—®é¢˜' æ¥å°è¯•å¯è§†åŒ–)")
        print(" è¾“å…¥ 'debug:æ‚¨çš„é—®é¢˜' æ¥æŸ¥çœ‹è¯¦ç»†çš„GraphRAGä¸Šä¸‹æ–‡ï¼ˆä»…è°ƒè¯•ç”¨ï¼‰")
        print("=" * 80)
        while True:
            raw_query = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
            if not raw_query: continue

            if raw_query.lower() in ['é€€å‡º', 'exit', 'quit']:
                print("æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            
            include_debug = False
            query_to_process = raw_query

            if raw_query.lower().startswith("å¯è§†åŒ–:"):
                query_to_visualize = raw_query[len("å¯è§†åŒ–:"):].strip()
                if query_to_visualize:
                    print(f"æ­£åœ¨ä¸ºæŸ¥è¯¢ '{query_to_visualize}' ç”ŸæˆçŸ¥è¯†å›¾å¯è§†åŒ–...")
                    self.visualize_knowledge_for_query(query_to_visualize)
                else:
                    print("è¯·è¾“å…¥è¦å¯è§†åŒ–çš„æŸ¥è¯¢å†…å®¹ã€‚")
                continue
            elif raw_query.lower().startswith("debug:"):
                include_debug = True
                query_to_process = raw_query[len("debug:"):].strip()
                print("--- è°ƒè¯•æ¨¡å¼å¼€å¯ ---")


            print("\næ­£åœ¨æ€è€ƒä¸­ï¼Œè¯·ç¨å€™...")
            start_time = time.time()
            
            final_answer = self.query(query_to_process, include_context_debug=include_debug) 
            
            end_time = time.time()

            print(f"\nğŸ’¡ æ™ºèƒ½åŠ©æ‰‹ (ç”¨æ—¶ {end_time - start_time:.2f}ç§’):")
            print("-" * 70)
            print(final_answer)
            print("-" * 70)

    def query(self, text: str, include_context_debug: bool = False) -> str: 
        graphrag_response, kg_context_str, kg_items, extracted_info = self.rag.generate_graphrag_response_only(text, include_context_debug=include_context_debug)
        
        if include_context_debug:
             print(f"\n--- å›¾è°±ç‰ˆå›ç­” (GraphRAG Kimi) ---\n{graphrag_response}")
             print(f"\n--- ç”¨äºå›¾è°±ç‰ˆå›ç­”çš„ä¸Šä¸‹æ–‡ ---\n{kg_context_str}")
             # print(f"\n--- æ£€ç´¢åˆ°çš„çŸ¥è¯†å›¾è°±æ¡ç›® ---\n{json.dumps(kg_items, ensure_ascii=False, indent=2)}")


        general_response = self.rag.get_general_kimi_response(text)
        # if include_context_debug: # è°ƒè¯•æ—¶æ‰“å°
            # print(f"\n--- é€šç”¨ç‰ˆå›ç­” (General Kimi) ---\n{general_response}")

        print("\n ç”Ÿæˆå›ç­”...")
        final_response = self.rag.synthesize_responses(text, graphrag_response, general_response)
        
        return final_response
    
    def visualize_knowledge_for_query(self, query: str):
        extracted_info = self.rag.extract_keywords_and_intent(query)
        keywords = extracted_info.get("keywords", [])
        
        entities_to_visualize = []
        for keyword in keywords:
            matched = self.kg.search_by_keyword(keyword)
            entities_to_visualize.extend(matched)
        
        if not entities_to_visualize:
            print(f"æœªèƒ½ä»æŸ¥è¯¢ '{query}' ä¸­æ‰¾åˆ°ç›´æ¥åŒ¹é…çš„å®ä½“è¿›è¡Œå¯è§†åŒ–ã€‚")
            # self.kg.visualize_graph()
            return

        unique_entities = list(set(entities_to_visualize))
        print(f"å°†å¯è§†åŒ–ä¸å®ä½“ {unique_entities} ç›¸å…³çš„å­å›¾...")
        self.kg.visualize_graph(unique_entities)



def main():
    # set your dataset here
    csv_path_from_previous_script = os.getenv("TCM_KG_CSV_PATH")

    if not os.path.exists(csv_path_from_previous_script):
        print(f"é”™è¯¯: æœªæ‰¾åˆ°çŸ¥è¯†å›¾è°±CSVæ–‡ä»¶ '{csv_path_from_previous_script}'ã€‚")
        print("è¯·ç¡®ä¿æ‚¨å·²è¿è¡Œä¹‹å‰çš„è„šæœ¬ç”Ÿæˆäº†è¯¥æ–‡ä»¶ï¼Œæˆ–è€…é€šè¿‡ TCM_KG_CSV_PATH ç¯å¢ƒå˜é‡æŒ‡å®šäº†æ­£ç¡®è·¯å¾„ã€‚")
        print("å¦‚æœéœ€è¦æµ‹è¯•ï¼Œæ‚¨å¯ä»¥æ‰‹åŠ¨åˆ›å»ºä¸€ä¸ªåŒ…å« Subject, Predicate, Object åˆ—çš„ç®€å•CSVæ–‡ä»¶ã€‚")
        use_sample_data = input(f"æ˜¯å¦åˆ›å»ºä¸€ä¸ªæ¼”ç¤ºç”¨çš„ç¤ºä¾‹CSVæ–‡ä»¶ '{csv_path_from_previous_script}' (yes/no)? ").lower()
        if use_sample_data == 'yes':
            print(f"æ­£åœ¨åˆ›å»ºä¸€ä¸ªç¤ºä¾‹CSVæ–‡ä»¶ '{csv_path_from_previous_script}' ç”¨äºæ¼”ç¤º...")
            sample_data = {
                'Subject': [
                    'æ²»ç–—æ–¹æ¡ˆ_é€é¥æ•£', 'æ²»ç–—æ–¹æ¡ˆ_é€é¥æ•£', 'æ²»ç–—æ–¹æ¡ˆ_é€é¥æ•£', 'æŸ´èƒ¡', 'å½“å½’', 'ç™½èŠ', 'ä¹³è…ºå¢ç”Ÿ',
                    'æ²»ç–—æ–¹æ¡ˆ_å…­å‘³åœ°é»„ä¸¸', 'æ²»ç–—æ–¹æ¡ˆ_å…­å‘³åœ°é»„ä¸¸', 'æ²»ç–—æ–¹æ¡ˆ_å…­å‘³åœ°é»„ä¸¸', 'ç†Ÿåœ°é»„',
                    'æœˆç»ä¸è°ƒ', 'æœˆç»ä¸è°ƒ', 'å¤´ç—›', 'ç”ŸåŒ–æ±¤', 'ç”ŸåŒ–æ±¤'
                ],
                'Predicate': [
                    'ä½¿ç”¨è‰è¯', 'ä½¿ç”¨è‰è¯', 'ä½¿ç”¨è‰è¯', 'å‰‚é‡', 'å‰‚é‡', 'å‰‚é‡', 'å¸¸è§ç—‡çŠ¶',
                    'æ²»ç–—ç–¾ç—…', 'ä½¿ç”¨è‰è¯', 'å¤‡æ³¨', 'ä¸»æ²»',
                    'å¯èƒ½ç—…å› ', 'æ¨èæ–¹å‰‚', 'å®šä¹‰æ²»ç–—æ–¹æ¡ˆ', 'ä½¿ç”¨è‰è¯', 'æ²»ç–—ç–¾ç—…'
                ],
                'Object': [
                    'æŸ´èƒ¡', 'å½“å½’', 'ç™½èŠ', '9å…‹', '9å…‹', '12å…‹', 'ä¹³æˆ¿èƒ€ç—›',
                    'è‚è‚¾é˜´è™š', 'ç†Ÿåœ°é»„', 'èœœä¸¸ï¼Œä¸€æ¬¡9å…‹ï¼Œä¸€æ—¥2æ¬¡', 'æ»‹é˜´è¡¥è‚¾',
                    'è‚æ°”éƒç»“', 'æ²»ç–—æ–¹æ¡ˆ_é€é¥æ•£', 'æ²»ç–—æ–¹æ¡ˆ_å¤´ç—›æ–¹', 'å½“å½’', 'äº§åè¡€ç˜€'
                ],

                'SourceBookName': ['æµ‹è¯•åŒ»ä¹¦'] * 16,
                'SourceChapterName': ['æµ‹è¯•ç« èŠ‚'] * 16
            }
            sample_df = pd.DataFrame(sample_data)
            sample_df.to_csv(csv_path_from_previous_script, index=False, encoding='utf-8-sig')
            print(f"ç¤ºä¾‹CSVæ–‡ä»¶ '{csv_path_from_previous_script}' å·²åˆ›å»ºã€‚è¯·ç”¨æ‚¨çš„å®é™…æ•°æ®æ›¿æ¢å®ƒä»¥è·å¾—æœ€ä½³æ•ˆæœã€‚")
        else:
            print("ç¨‹åºå°†é€€å‡ºï¼Œå› ä¸ºç¼ºå°‘å¿…è¦çš„è¾“å…¥æ–‡ä»¶ã€‚")
            return

    app = TCMGraphRAGApp(csv_path_from_previous_script)
    app.run_interactive_cli()

if __name__ == "__main__":
    main()